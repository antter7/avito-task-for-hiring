{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cufflinks as cf\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pymorphy2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, pyll\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "cf.go_offline()\n",
    "%matplotlib inline\n",
    "RS = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e60aa02955bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stop_words_ru.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "load = pd.read_csv('val.csv')\n",
    "with open(\"stop_words_ru.txt\", encoding='utf-8') as f:\n",
    "    stop_words = [x.replace(\"\\n\", \"\") for x in f] \n",
    "load.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем категориальные переменные \n",
    "y = load.is_bad\n",
    "data = pd.DataFrame(load.loc[:, ('price')])\n",
    "load.datetime_submitted = pd.to_datetime(load.datetime_submitted)\n",
    "load['weekday_submitted'] = load.datetime_submitted.dt.weekday\n",
    "load['month_submitted'] =  load.datetime_submitted.dt.month\n",
    "load['year_submitted'] =  load.datetime_submitted.dt.year\n",
    "d = pd.get_dummies(load[['subcategory', 'category', 'region', 'city', 'weekday_submitted', 'month_submitted', \n",
    "                         'year_submitted']].astype('str'))\n",
    "data = data.join(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = text.split()\n",
    "    res = list()\n",
    "    int_list = []\n",
    "    for word in words:\n",
    "        if any(i.isdigit() for i in word) and len(word) > 3:\n",
    "            int_word = ''\n",
    "            for w in word:\n",
    "                if w.isdigit():\n",
    "                    int_word = int_word + w \n",
    "            int_list.append(int_word)\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "    res = res + int_list\n",
    "    res = [i for i in res if i not in stop_words]\n",
    "    res = ' '.join(res)\n",
    "    return res        \n",
    "\n",
    "def del_s(_text):\n",
    "    punctuation = \"!#$%^&*()_+<>?:.,;/-\"    \n",
    "    for c in _text:\n",
    "        if c in punctuation:\n",
    "            _text = _text.replace(c, \"\")\n",
    "    return _text\n",
    "\n",
    "data['new_description'] = load.description.apply(lambda x: del_s(x))\n",
    "data['new_description'] = data['new_description'].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_(string, rule):\n",
    "    if re.search(rule, string):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def count_int(text):\n",
    "    counter = 0\n",
    "    for t in text:\n",
    "        if t.isdigit():\n",
    "            counter += 1\n",
    "    return counter\n",
    "    \n",
    "tel = r'[\\+]?[(-]?[0-9]{3}[-)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}'\n",
    "sabaka = '@'\n",
    "data['telephone'] = data['new_description'].apply(lambda x: find_(x, tel))\n",
    "data['@'] = data['new_description'].apply(lambda x: find_(x, sabaka))\n",
    "data['count_int'] = data['new_description'].apply(lambda x: count_int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=.2, stratify=y, random_state=RS)\n",
    "X_test, X_control, y_test, y_control = train_test_split(X_test, y_test, test_size=.5, stratify=y_test, random_state=RS)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "values = tfidf.fit_transform(X_train.new_description)\n",
    "feature_names = tfidf.get_feature_names()\n",
    "X_train = X_train.join(pd.DataFrame(values.toarray(), columns=feature_names))\n",
    "\n",
    "values_test = tfidf.transform(X_test.new_description)\n",
    "X_test = X_test.join(pd.DataFrame(values_test.toarray(), columns=feature_names))\n",
    "values_control = tfidf.transform(X_control.new_description)\n",
    "X_control = X_control.join(pd.DataFrame(values_control.toarray(), columns=feature_names))\n",
    "train = X_train.drop('new_description', axis=1).fillna(0)\n",
    "test =  X_test.drop('new_description', axis=1).fillna(0)\n",
    "control = X_control.drop('new_description', axis=1).fillna(0)\n",
    "\n",
    "# нормализуем для линейных методов\n",
    "scaler = StandardScaler()\n",
    "norm_train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns) \n",
    "norm_test = pd.DataFrame(scaler.transform(test), columns=train.columns)\n",
    "norm_control = pd.DataFrame(scaler.transform(control), columns=train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_distance = [lgb.LGBMClassifier().__class__.__name__, xgb.XGBClassifier().__class__.__name__,]\n",
    "\n",
    "# подбираем гиперпараметры и модель\n",
    "methods = [\n",
    "# 'LR',\n",
    "# 'SGDC',\n",
    "# 'LGB',\n",
    "'XGB',\n",
    "# 'MLPC'\n",
    "]\n",
    "for i in methods:\n",
    "    if __name__ == '__main__':\n",
    "        def hyperopt_train_test(params):\n",
    "            if i == 'LR':\n",
    "                X = norm_train\n",
    "                model = LogisticRegression(n_jobs=5,  **params)     \n",
    "            elif i == 'SGDC':\n",
    "                X = norm_train\n",
    "                model = SGDClassifier(n_jobs=5, early_stopping=True, **params)\n",
    "            elif i == 'LGB':\n",
    "                p = params.pop('eval_metric')\n",
    "                X = train\n",
    "                model = lgb.LGBMClassifier(n_jobs=5, objective='binary', **params)\n",
    "            elif i == 'XGB':\n",
    "                p = params.pop('eval_metric')\n",
    "                X = train\n",
    "                model = xgb.XGBClassifier(n_jobs=5, **params)\n",
    "            elif i == 'MLPC':\n",
    "                X = norm_train\n",
    "                model = MLPClassifier(early_stopping=True, **params)\n",
    "            \n",
    "            if i in ('LGB', 'XGB'):\n",
    "                eval_set = [(test.values, y_test)]\n",
    "                model.fit(X.values, y_train, eval_set=eval_set, eval_metric=p, early_stopping_rounds=20, verbose=False)\n",
    "            else:\n",
    "                model.fit(X.values, y_train)\n",
    "            \n",
    "            if  model.__class__.__name__ in not_distance:\n",
    "                y_pred = model.predict_proba(test.values)\n",
    "                y_pred_control = model.predict_proba(control.values)                    \n",
    "            else:\n",
    "                y_pred = model.predict_proba(norm_test.values)\n",
    "                y_pred_control = model.predict_proba(norm_control.values)\n",
    "                \n",
    "            y_pred_1d = [x[1] for x in y_pred]\n",
    "            y_pred_control_1d = [x[1] for x in y_pred_control]\n",
    "            \n",
    "            f1_t, f1_c = roc_auc_score(y_test, y_pred_1d), roc_auc_score(y_control, y_pred_control_1d)\n",
    "            return f1_t, f1_c\n",
    "        \n",
    "        n_features = train.shape[1]\n",
    "        min_features = 1 / n_features\n",
    "        \n",
    "        if i == 'LR':\n",
    "            space = {\n",
    "                'class_weight': hp.choice('class_weight', (None, 'balanced')), \n",
    "                'penalty': hp.choice('penalty', ('l2', 'none')),\n",
    "                'C': 10 ** hp.quniform('C', -3, 3, 0.5),\n",
    "                'solver': hp.choice('metric', ('lbfgs', 'sag', 'saga')),\n",
    "                'max_iter': hp.quniform('max_iter', 50, 1000, 50),\n",
    "            }\n",
    "            step = 300\n",
    "            \n",
    "#         elif i == 'KNN':\n",
    "#             space = {\n",
    "#                 'n_neighbors': scope.int(hp.quniform('n_neighbors', 2, 30, 1)),\n",
    "#                 'weights': hp.choice('weights', ('uniform', 'distance')),\n",
    "#                 'p': hp.quniform('p', 1, 20, 1),\n",
    "#                 'metric': hp.choice('metric', ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', \n",
    "#                                                'braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', \n",
    "#                                                'hamming', 'jaccard', 'kulsinski', 'minkowski', 'rogerstanimoto', \n",
    "#                                                'russellrao', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']),\n",
    "#             }\n",
    "#             step = 300            \n",
    "        \n",
    "        elif i == 'SGDC':\n",
    "            space = {\n",
    "                'loss': hp.choice('loss', ('log', 'modified_huber', \n",
    "#                                            'hinge', 'squared_hinge', 'perceptron'\n",
    "                                          )),\n",
    "                'alpha': 10 ** hp.quniform('alpha', -6, -1, 1),\n",
    "                'penalty': hp.choice('penalty', ('none', 'l2', 'l1', 'elasticnet')),\n",
    "                'l1_ratio': hp.quniform('l1_ratio', 0.05, 0.95, 0.05),\n",
    "                'max_iter': scope.int(hp.quniform('n_estimators', 500, 2000, 50)),\n",
    "                'learning_rate': hp.choice('learning_rate', ('constant', 'optimal', 'invscaling')),\n",
    "                'eta0': 10 ** hp.quniform('eta0', -6, -1, 1),\n",
    "                'power_t': hp.quniform('power_t', 0.05, 0.95, 0.05),\n",
    "                'class_weight': hp.choice('class_weight', ['balanced', None]),\n",
    "            }\n",
    "            step = 500\n",
    "\n",
    "#         elif i == 'RF':\n",
    "#             space = {\n",
    "#                 'n_estimators': scope.int(hp.quniform('n_estimators', 50, 1200, 50)),\n",
    "#                 'max_depth': hp.choice('max_depth', [x for x in range(1, 51)] + [None]),\n",
    "#                 'min_samples_split': hp.quniform('min_samples_split', 0.05, 1, 0.05),\n",
    "#                 'min_samples_leaf': hp.quniform('min_samples_leaf', 0.05, 0.5, 0.05),\n",
    "#                 'min_weight_fraction_leaf': hp.quniform('min_weight_fraction_leaf', 0.05, 0.5, 0.05),\n",
    "#                 'max_features': hp.quniform('max_features', min_features, 1, min_features),\n",
    "#                 'class_weight': hp.choice('class_weight', ['balanced', 'balanced_subsample', None])\n",
    "#             }\n",
    "#             step = 300\n",
    "            \n",
    "        elif i == 'LGB':\n",
    "            space = {\n",
    "                'boosting_type': hp.choice('boosting_type', ('gbdt', 'dart')),\n",
    "                'num_leaves': scope.int(hp.quniform('num_leaves', 2, 100, 1)),\n",
    "                'max_depth': hp.choice('max_depth', [x for x in range(1, 51)] + [-1]),\n",
    "                'learning_rate': hp.choice('learning_rate', np.logspace(-3, -0.5, 49)),\n",
    "                'n_estimators': scope.int(hp.quniform('n_estimators', 100, 2000, 50)),\n",
    "#                 'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 400000, 20000),\n",
    "#                 'objective': hp.choice('objective', ['multiclass', 'multiclassova']),\n",
    "                'class_weight': hp.choice('class_weight', ['balanced', None]),\n",
    "                'min_child_samples': scope.int(hp.quniform('min_child_samples', 5, 200, 5)),\n",
    "                'subsample': hp.quniform('subsample', 0.7, 1, 0.05),\n",
    "                'subsample_freq': scope.int(hp.quniform('subsample_freq', 1, 100, 1)),\n",
    "                'colsample_bytree': hp.quniform('colsample_bytree', min_features, 1, min_features),\n",
    "                'reg_alpha': hp.choice('reg_alpha', np.logspace(-3, 2, 20)),\n",
    "                'reg_lambda': hp.choice('reg_lambda', np.logspace(-3, 2, 20)),\n",
    "                'eval_metric': hp.choice('eval_metric', ['None', 'auc', 'binary_logloss', 'binary_error'])\n",
    "            }\n",
    "            step = 700\n",
    "        elif i == 'XGB':\n",
    "            space = {\n",
    "                'max_depth': scope.int(hp.quniform('max_depth', 1, 50, 1)),\n",
    "                'learning_rate': hp.loguniform('learning_rate', -3, -0.3),\n",
    "                'n_estimators': scope.int(hp.quniform('n_estimators', 100, 2000, 50)),\n",
    "                'objective': hp.choice('objective', ['binary:logistic', 'binary:logitraw', 'binary:hinge']),\n",
    "                'booster': hp.choice('booster', ['gbtree', \n",
    "#                                                  'gblinear', # Check failed: ntree_limit == 0U (1 vs. 0) \n",
    "                                                 'dart']),\n",
    "                'gamma': hp.quniform('gamma', 0, 10, 0.05),\n",
    "                'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 50, 1)),\n",
    "                'max_delta_step': scope.int(hp.quniform('max_delta_step', 0, 50, 1)),\n",
    "                'subsample': hp.quniform('subsample', 0.7, 1, 0.05),\n",
    "                'colsample_bytree': hp.quniform('colsample_bytree', min_features, 1, min_features),\n",
    "                'colsample_bylevel': hp.quniform('colsample_bylevel', min_features, 1, min_features),\n",
    "                'reg_alpha': hp.choice('reg_alpha', np.logspace(-4, 2, 31)),\n",
    "                'reg_lambda': hp.choice('reg_lambda', np.logspace(-4, 2, 31)),\n",
    "                'eval_metric': hp.choice('eval_metric', ['logloss',  'error', 'auc', 'map'])\n",
    "            }\n",
    "            step = 500\n",
    "            \n",
    "        elif i == 'MLPC':\n",
    "            space = {\n",
    "                'activation': hp.choice('activation', ['relu', 'logistic', 'tanh']),\n",
    "                'solver': hp.choice('solver', ['sgd', 'adam']),\n",
    "                'alpha': hp.loguniform('alpha', 1e-6, 1e-1),\n",
    "                'batch_size': hp.choice('batch_size', [x for x in range(50, 700, 10)] + ['auto']),\n",
    "                'learning_rate': hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive']),\n",
    "                'learning_rate_init': hp.loguniform('learning_rate_init', 1e-4, 5e-1),\n",
    "                'power_t': hp.quniform('power_t', 0.05, 0.95, 0.05),\n",
    "                'max_iter': hp.quniform('max_iter', 50, 700, 50),\n",
    "                'momentum': hp.quniform('momentum', 0.05, 0.95, 0.05),\n",
    "                'beta_1': hp.quniform('beta_1', 0.05, 0.99, 0.05),\n",
    "                'beta_2': hp.quniform('beta_2', 0.05, 0.99, 0.05),\n",
    "                'hidden_layer_sizes': hp.quniform('hidden_layer_sizes', \n",
    "                                                  (hp.uniform('size', 10, 500, 10),) * hp.uniform('layer', 1, 10, 1))\n",
    "            }\n",
    "            step = 500            \n",
    "            \n",
    "        count = 0\n",
    "        best = 0\n",
    "\n",
    "\n",
    "        def f(params):\n",
    "            global best, count\n",
    "            count += 1\n",
    "            t = hyperopt_train_test(params.copy())[0]\n",
    "            cont = hyperopt_train_test(params.copy())[1]\n",
    "            acc = np.mean((t, cont))\n",
    "            if acc > best:\n",
    "                print('new best: {},'.format(str(round(acc * 100, 5))), \n",
    "                      'test {}, control {}'.format(str(round(t * 100, 5)), str(round(cont * 100, 5))), 'using {}'.format(i), params, sep='  ')\n",
    "                best = acc\n",
    "#             if count % 30 == 0:\n",
    "#                 end_time = datetime.now()\n",
    "#                 print('iters:', count, 'duration: {}'.format(end_time - start_time), sep=' ')\n",
    "            return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "        trials = Trials()\n",
    "        best = fmin(f, space, algo=tpe.suggest, max_evals=step, trials=trials)\n",
    "        print('BEST', best)\n",
    "        print(min(trials.losses()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1786"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups = datasets.fetch_20newsgroups(subset='all', categories=['alt.atheism', 'sci.space'])\n",
    "values = tfidf.fit_transform(newsgroups.data)\n",
    "feature_names = tfidf.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)\n",
    "len(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperopt.pyll.base.Apply at 0x1b439ad1788>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.quniform('boosting_type', (hp.uniform('size', 10, 500, 10),) * hp.uniform('layer', 1, 10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7524746489380016"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SGDClassifier(n_jobs=5, loss='log')\n",
    "m.fit(norm_train, y_train)\n",
    "roc_auc_score(y_test, [x[1] for x in m.predict_proba(norm_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new best: 93.16894,                                                                                                    \n",
    "test 92.69943, control 93.63846                                                                                        \n",
    "using LGB                                                                                                              \n",
    "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.7524080214748146, 'eval_metric': 'auc', 'learning_rate': 0.01778279410038923, 'max_depth': 49, 'min_child_samples': 5, 'n_estimators': 1150, 'num_leaves': 85, 'reg_alpha': 0.003359818286283781, 'reg_lambda': 0.12742749857031335, 'subsample': 0.9500000000000001, 'subsample_freq': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-adc701d42449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "params = {'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.7524080214748146, 'eval_metric': 'auc', \n",
    "          'learning_rate': 0.01778279410038923, 'max_depth': 49, 'min_child_samples': 5, 'n_estimators': 1150, \n",
    "          'num_leaves': 85, 'reg_alpha': 0.003359818286283781, 'reg_lambda': 0.12742749857031335, \n",
    "          'subsample': 0.95, 'subsample_freq': 6}\n",
    "model = lgb.LGBMClassifier(n_jobs=4, objective='binary', **params)\n",
    "model.fit(train.values, y_train)\n",
    "roc_auc_score(y_test, [x[1] for x in model.predict_proba(test.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9358538204890685"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_control, [x[1] for x in model.predict_proba(control.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x21e9d7b5c88>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.booster_.save_model('lgb_classifier.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9358538204890685"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = lgb.Booster(model_file='lgb_classifier.txt')\n",
    "roc_auc_score(y_control, clf_fs.predict(control.values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
